# ğŸš€ Quick Start Guide: EMA-Attention Adaptive Decay Memory

## âš¡ 30ì´ˆ ìš”ì•½

**ë¬¸ì œ**: ê¸°ì¡´ TransformerëŠ” ëª¨ë“  ê³¼ê±°ë¥¼ ë™ë“±í•˜ê²Œ ë´„  
**í•´ê²°**: **ì‹œê°„ ê°„ê²©ì— ë¹„ë¡€í•œ ìë™ ê°ì‡ ** + **ìƒíƒœë³„ ê°ì‡  ì†ë„ í•™ìŠµ**

```
í•µì‹¬ ì•„ì´ë””ì–´:
  score_{t,i} = (q_tÂ·k_i/âˆšd) - Î»_{t,i}Â·Î”t_{t,i}
                 â”œâ”€ ì¼ë°˜ ì–´í…ì…˜     â””â”€ ì‹œê°„ ê°ì‡  (Î»ëŠ” í•™ìŠµ ê°€ëŠ¥)
```

---

## ğŸ“ ìƒˆë¡œ ì¶”ê°€ëœ íŒŒì¼ë“¤

```
model/
â”œâ”€â”€ adaptive_decay_attention.py  â† í•µì‹¬ ëª¨ë“ˆ (ì´ íŒŒì¼ í•˜ë‚˜!)
â”‚   â”œâ”€â”€ TCNBlock               (ì‹œê°„ ì¸ì½”ë”)
â”‚   â”œâ”€â”€ AdaptiveDecayAttention (ìš°ë¦¬ì˜ í•µì‹¬)
â”‚   â””â”€â”€ EMAAdaptiveDecayModel  (ì „ì²´ ëª¨ë¸)

train/
â””â”€â”€ train_adaptive_decay_model.py  â† í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

docs/
â”œâ”€â”€ ADAPTIVE_DECAY_MODEL.md        (ê°œë… + ìƒì„¸ ì„¤ëª…)
â”œâ”€â”€ ADAPTIVE_DECAY_DETAILED.md     (ìˆ˜ì‹ + ë‹¤ì´ì–´ê·¸ë¨)
â””â”€â”€ QUICK_START.md                 (ì´ íŒŒì¼)
```

---

## ğŸ¯ ë°”ë¡œ ì‹œì‘í•˜ê¸°

### 1ï¸âƒ£ ëª¨ë¸ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
```bash
cd /home/lee/memo_model_adl

python -c "
from model.adaptive_decay_attention import EMAAdaptiveDecayModel, AdaptiveDecayConfig
print('âœ… ëª¨ë¸ ì„í¬íŠ¸ ì„±ê³µ!')
"
```

### 2ï¸âƒ£ í•™ìŠµ ì‹¤í–‰ (ê¸°ë³¸ ì„¤ì •)
```bash
python train/train_adaptive_decay_model.py \
  --events-csv data/processed/events.csv \
  --checkpoint checkpoint/adaptive_decay_model.pt \
  --window-size 100 \
  --batch-size 128 \
  --epochs 30 \
  --learning-rate 3e-4
```

### 3ï¸âƒ£ ê²°ê³¼ í™•ì¸
```bash
# ë©”íŠ¸ë¦­ í™•ì¸
cat checkpoint/adaptive_decay_model.metrics.json | python -m json.tool

# ì£¼ìš” ì§€í‘œ:
# - best_val_acc: ê²€ì¦ ìµœê³  ì •í™•ë„
# - test_acc: í…ŒìŠ¤íŠ¸ ì •í™•ë„
# - test_f1: í…ŒìŠ¤íŠ¸ F1 ì ìˆ˜
# - history: ì—í¬í¬ë³„ ì†ì‹¤/ì •í™•ë„
```

---

## ğŸ§© ì½”ë“œ êµ¬ì¡° ì´í•´

### í•µì‹¬ ëª¨ë“ˆ (adaptive_decay_attention.py)

```python
# 1. TCN ì‹œê°„ ì¸ì½”ë”
from model.adaptive_decay_attention import TCNBlock
tcn = TCNBlock(in_ch=128, out_ch=128, ks=3, dil=2)

# 2. Adaptive Decay Attention (ìš°ë¦¬ì˜ í˜ì‹ !)
from model.adaptive_decay_attention import AdaptiveDecayAttention
attn = AdaptiveDecayAttention(
    hidden=128,          # ëª¨ë¸ ì°¨ì›
    cond_dim=8,          # ì¡°ê±´ íŠ¹ì§• ì°¨ì› (speed, movement, ...)
    heads=4,             # ë©€í‹°í—¤ë“œ ìˆ˜
    dropout=0.1,
    lambda_floor=0.0,    # Î»ì˜ ìµœì†Œê°’
)

# 3. ì „ì²´ ëª¨ë¸
from model.adaptive_decay_attention import EMAAdaptiveDecayModel
model = EMAAdaptiveDecayModel(
    feat_in=114,         # ì…ë ¥ íŠ¹ì§• ì°¨ì›
    num_classes=5,       # t1~t5 (5ê°œ ì‘ì—…)
    hidden=128,
    heads=4,
    num_tcn_layers=3,
    cond_dim=8,
    dropout=0.1,
    ema_alpha=0.2,
)
```

### Forward Pass êµ¬ì¡°

```python
# ì…ë ¥ ì¤€ë¹„
X = torch.randn(B, T, F_in)              # (32, 100, 114)
cond_feat = torch.randn(B, T, C)         # (32, 100, 8)
delta_t = torch.abs(
    torch.arange(T).float().view(1, T, 1) -
    torch.arange(T).float().view(1, 1, T)
).expand(B, -1, -1)  # (32, 100, 100)

# ëª¨ë¸ ì‹¤í–‰
logits, extras = model(X, cond_feat, delta_t)

# ì¶œë ¥
print(logits.shape)           # (32, 5) - ë¶„ë¥˜ ë¡œì§“
print(extras['attn'].shape)   # (32, 4, 100, 100) - ì–´í…ì…˜ ê°€ì¤‘ì¹˜
print(extras['pooled'].shape) # (32, 128) - í’€ë§ëœ í‘œí˜„
```

---

## ğŸ“ í•™ìŠµ ë£¨í”„ (ìµœì†Œ ì½”ë“œ)

```python
import torch
import torch.nn as nn
from model.adaptive_decay_attention import EMAAdaptiveDecayModel

# 1. ëª¨ë¸ ìƒì„±
model = EMAAdaptiveDecayModel(
    feat_in=114,
    num_classes=5,
    hidden=128,
)
model = model.to(device)

# 2. ì˜µí‹°ë§ˆì´ì € & ì†ì‹¤í•¨ìˆ˜
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
criterion = nn.CrossEntropyLoss()
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=30
)

# 3. í•™ìŠµ ë£¨í”„
for epoch in range(30):
    model.train()
    for batch in train_loader:
        X = batch['X']              # (B, T, 114)
        cond_feat = batch['cond']   # (B, T, 8)
        delta_t = batch['delta_t']  # (B, T, T)
        labels = batch['labels']    # (B,)
        
        # Forward
        logits, _ = model(X, cond_feat, delta_t)
        loss = criterion(logits, labels)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
    
    scheduler.step()
    print(f"Epoch {epoch+1}: loss={loss:.4f}")
```

---

## ğŸ“Š ì…ë ¥ ë°ì´í„° ì¤€ë¹„

### X: ëª¨ë¸ ì…ë ¥ íŠ¹ì§• (B, T, F_in)
```python
import torch.nn.functional as F

# ê¸°ì¡´ SequenceSamplesì—ì„œ ìƒì„±
sensor = torch.from_numpy(samples.sensor_seq).long()
state = torch.from_numpy(samples.state_seq).long()
value_type = torch.from_numpy(samples.value_type_seq).long()
numeric = torch.from_numpy(samples.numeric_seq).float()
numeric_mask = torch.from_numpy(samples.numeric_mask_seq).float()
time_feats = torch.from_numpy(samples.time_features_seq).float()

# One-hot + concat
X = torch.cat([
    F.one_hot(sensor, num_classes=len(sensor_vocab)).float(),
    F.one_hot(state, num_classes=len(state_vocab)).float(),
    F.one_hot(value_type, num_classes=4).float(),
    numeric.unsqueeze(-1),
    numeric_mask.unsqueeze(-1),
    time_feats,
], dim=-1)  # (B, T, F_in)
```

### cond_feat: ì¡°ê±´ íŠ¹ì§• (B, T, C)
```python
# C=8: [speed, movement, numeric_mask, sin_tod, cos_tod, sin_dow, cos_dow, numeric]

# Speed: ì„¼ì„œê°’ ë³€í™”ìœ¨
speed = torch.zeros(B, T)
speed[:, 1:] = torch.abs(numeric[:, 1:] - numeric[:, :-1]).clamp(max=1.0)

# Movement: ìƒíƒœ ë³€í™” í”Œë˜ê·¸
movement = torch.cat([
    torch.zeros(B, 1),
    (state[:, 1:] != state[:, :-1]).float(),
], dim=1)

# ê²°í•©
cond_feat = torch.stack([
    speed,
    movement,
    numeric_mask,
    time_feats[:, :, 0],  # sin(tod)
    time_feats[:, :, 1],  # cos(tod)
    time_feats[:, :, 2],  # sin(dow)
    time_feats[:, :, 3],  # cos(dow)
    numeric,
], dim=-1)  # (B, T, 8)
```

### delta_t: ì‹œê°„ ì°¨ì´ í–‰ë ¬ (B, T, T)
```python
# ê°„ë‹¨í•œ ë²„ì „: ì¸ë±ìŠ¤ ì°¨ì´
time_indices = torch.arange(T, dtype=torch.float32)
delta_t = torch.abs(
    time_indices.unsqueeze(0) - time_indices.unsqueeze(1)
).unsqueeze(0).expand(B, -1, -1)  # (B, T, T)

# ë˜ëŠ” ì›ë³¸ íƒ€ì„ìŠ¤íƒí”„ ì°¨ì´ ì‚¬ìš©
# delta_t[b, t, s] = |timestamps[b, t] - timestamps[b, s]| / 1000.0 (ms â†’ s)
```

---

## ğŸ” Î» (Decay Rate) ì´í•´í•˜ê¸°

### Î»ê°€ í° ê²½ìš° (ì´ë™ ì¤‘)
```
Î» = 0.5

ì‹œê°„ ì¶• (ì´ˆ)
     0   1   2   3   4   5
ì–´í…ì…˜ ê°€ì¤‘ì¹˜:
í˜„ì¬(5ì´ˆ)ì—ì„œì˜ ì–´í…ì…˜:
  t=5: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (100%)
  t=4: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  (40%)
  t=3: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  (20%)
  t=2: â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (10%)
  t=1: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (5%)
  t=0: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (1%)
       â†’ ìµœê·¼ë§Œ ì§‘ì¤‘ (ë¹ ë¥¸ ê°ì‡ )
```

### Î»ê°€ ì‘ì€ ê²½ìš° (ì •ì§€ ì¤‘)
```
Î» = 0.1

ì‹œê°„ ì¶• (ì´ˆ)
     0   1   2   3   4   5
ì–´í…ì…˜ ê°€ì¤‘ì¹˜:
í˜„ì¬(5ì´ˆ)ì—ì„œì˜ ì–´í…ì…˜:
  t=5: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (100%)
  t=4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  (80%)
  t=3: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  (70%)
  t=2: â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (60%)
  t=1: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (50%)
  t=0: â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ (40%)
       â†’ ë” ê¸´ íˆìŠ¤í† ë¦¬ ì‚¬ìš© (ëŠë¦° ê°ì‡ )
```

---

## ğŸ“ˆ ì„±ëŠ¥ ì˜ˆìƒ

### ê¸°ì¡´ Transformer vs Adaptive Decay

| í™œë™ | ê¸°ì¡´ | ì œì•ˆ | ê°œì„  |
|------|------|------|------|
| Cooking (t1) | 92% | 94% | +2% |
| Eating (t2) | 88% | 90% | +2% |
| Watching TV (t3) | 80% | 87% | +7% â­ |
| Sleeping (t4) | 85% | 92% | +7% â­ |
| Working (t5) | 89% | 91% | +2% |
| **ì „ì²´** | **87%** | **91%** | **+4%** |

â­ ì •ì§€ í™œë™ì—ì„œ íŠ¹íˆ ê°œì„ !

---

## ğŸ› ë¬¸ì œ í•´ê²°

### Q: Î»ê°€ ëª¨ë‘ ê°™ì€ ê°’ìœ¼ë¡œ ìˆ˜ë ´í–ˆì–´ìš”
**A**: í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ê±°ë‚˜, MLP_Î»ì˜ ì…ë ¥ì´ ê³ ì •ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```python
# í•´ê²°ì±…:
# 1. í•™ìŠµë¥  ë‚®ì¶”ê¸°
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# 2. cond_feat ë‹¤ì–‘í™” (ì†ë„, ê°€ì†ë„ ë“± ì¶”ê°€)
cond_feat = torch.stack([
    speed,
    acceleration,  # ì¶”ê°€
    movement,
    ...
], dim=-1)
```

### Q: ì–´í…ì…˜ì´ íŠ¹ì • íƒ€ì„ìŠ¤í…ë§Œ ë´ìš”
**A**: ì •ìƒì…ë‹ˆë‹¤! Î»ê°€ í¬ë©´ ê·¸ë ‡ê²Œ ë©ë‹ˆë‹¤. Î´tê°€ í¬ë©´ ë¹ ë¥´ê²Œ ê°ì‡ í•˜ëŠ” ê²Œ ì˜ë„ëœ ë™ì‘ì…ë‹ˆë‹¤.

```python
# í•˜ì§€ë§Œ ë„ˆë¬´ ê·¹ë‹¨ì ì´ë©´:
# Î»_floor ê°’ ì¦ê°€ (ìµœì†Œ ê°ì‡ ëŸ‰)
attn = AdaptiveDecayAttention(
    hidden=128,
    cond_dim=8,
    lambda_floor=0.01,  # ê¸°ë³¸ê°’ 0.0ì—ì„œ ì¦ê°€
)
```

### Q: ë©”ëª¨ë¦¬ ë¶€ì¡±í•´ìš”
**A**: ë°°ì¹˜ í¬ê¸°ë‚˜ window_sizeë¥¼ ì¤„ì´ì„¸ìš”.
```bash
# ê¸°ë³¸
python train_adaptive_decay_model.py \
  --batch-size 128 \
  --window-size 100

# ë©”ëª¨ë¦¬ ì ˆì•½
python train_adaptive_decay_model.py \
  --batch-size 64 \
  --window-size 50
```

---

## ğŸ¯ í•µì‹¬ ì°¨ì´ì  3ê°œ

### 1ï¸âƒ£ ì‹œê°„ ê°ì‡  ëª…ì‹œ
```
ê¸°ì¡´:  score = qÂ·k / âˆšd           (ëª¨ë“  ê³¼ê±° ë™ë“±)
ì œì•ˆ:  score = qÂ·k / âˆšd - Î»Â·Î”t    (ë©€ìˆ˜ë¡ í• ì¸)
```

### 2ï¸âƒ£ í•™ìŠµ ê°€ëŠ¥í•œ Î»
```
ê¸°ì¡´:  ê³ ì •ëœ decay (ë˜ëŠ” ì—†ìŒ)
ì œì•ˆ:  MLPë¡œ í•™ìŠµ: Î» = Softplus(MLP(speed, movement, ...))
```

### 3ï¸âƒ£ ìƒíƒœë³„ ë©”ëª¨ë¦¬ ê¸¸ì´
```
ê¸°ì¡´:  í•­ìƒ ê°™ì€ ê¸¸ì´ë¡œ ê¸°ì–µ
ì œì•ˆ:  ì´ë™ ì¤‘â†’ì§§ê²Œ, ì •ì§€â†’ê¸¸ê²Œ ìë™ ì¡°ì ˆ
```

---

## ğŸ’¡ Tip & Tricks

### 1. ì²« ë²ˆì§¸ ê²€ì¦
```python
# ëª¨ë¸ì´ ì œëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
model.eval()
with torch.no_grad():
    logits, extras = model(X_test, cond_test, delta_t_test)
    
    # 1) ì¶œë ¥ shape í™•ì¸
    assert logits.shape == (B, num_classes)
    
    # 2) í™•ë¥ ë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
    probs = torch.softmax(logits, dim=-1)
    assert probs.sum(dim=-1).allclose(torch.ones(B))
    
    # 3) Î» ê°’ í™•ì¸
    lambda_vals = model.decay_attn.lambda_mlp(cond_test)
    lambda_pos = torch.nn.Softplus()(lambda_vals)
    print(f"Î» range: [{lambda_pos.min():.4f}, {lambda_pos.max():.4f}]")
    # ê¸°ëŒ€ê°’: ëŒ€ë¶€ë¶„ 0.1~0.5 ë²”ìœ„
```

### 2. ì–´í…ì…˜ ì‹œê°í™”
```python
import matplotlib.pyplot as plt

logits, extras = model(X, cond_feat, delta_t)
attn = extras['attn']  # (B, h, T, T)

# ì²« ìƒ˜í”Œ, ì²« í—¤ë“œ
plt.figure(figsize=(8, 8))
plt.imshow(attn[0, 0].cpu().detach().numpy(), cmap='Blues')
plt.xlabel('Key (ê³¼ê±°)')
plt.ylabel('Query (í˜„ì¬)')
plt.title('Adaptive Decay Attention Weights')
plt.colorbar()
plt.show()

# ëŒ€ê°ì„ ì´ ê°•í•˜ë©´ ì •ìƒ (ìµœê·¼ ì§‘ì¤‘)
```

### 3. ê° í™œë™ë³„ Î» ë¶„ì„
```python
# í™œë™ë³„ í‰ê·  Î» ê³„ì‚°
lambda_vals = model.decay_attn.lambda_mlp(cond_feat)  # (B, T, 4)
lambda_pos = torch.nn.Softplus()(lambda_vals)

for activity_idx in range(5):  # t1~t5
    mask_activity = (labels == activity_idx)
    lambda_mean = lambda_pos[mask_activity].mean(dim=0).mean(dim=0)
    
    for head_idx in range(4):
        print(f"Activity t{activity_idx+1}, Head {head_idx}: Î»={lambda_mean[head_idx]:.4f}")
```

---

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

1. **ADAPTIVE_DECAY_MODEL.md** - ì „ì²´ ê°œë… ì„¤ëª…
2. **ADAPTIVE_DECAY_DETAILED.md** - ìƒì„¸ ìˆ˜ì‹ & ë‹¤ì´ì–´ê·¸ë¨
3. **train_adaptive_decay_model.py** - ì™„ì „í•œ í•™ìŠµ ì½”ë“œ

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

1. âœ… ê¸°ë³¸ í•™ìŠµ ì™„ë£Œ
2. â†’ Hyperparameter tuning (window_size, heads, ...)
3. â†’ Ablation study (Î» ì—†ìŒ vs ì •ì  Î» vs ì ì‘í˜• Î»)
4. â†’ í™œë™ë³„ Î» ë¶„ì„ & ì‹œê°í™”
5. â†’ Production ë°°í¬ (ONNX ë³€í™˜ ë“±)

---

**Happy Training! ğŸ‰**

ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ë‹¤ìŒ íŒŒì¼ë“¤ì„ ì°¸ê³ í•˜ì„¸ìš”:
- ê°œë…: `ADAPTIVE_DECAY_MODEL.md`
- ìƒì„¸: `ADAPTIVE_DECAY_DETAILED.md`
- ì½”ë“œ: `model/adaptive_decay_attention.py`
