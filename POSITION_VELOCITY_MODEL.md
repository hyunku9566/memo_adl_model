# Position-Velocity-MMU/CMU Model

**í•™ìŠµ ê°€ëŠ¥í•œ ì„¼ì„œ ìœ„ì¹˜ + ì´ì¤‘ ë©”ëª¨ë¦¬ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸í™ˆ í™œë™ ì¸ì‹ ëª¨ë¸**

---

## ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´

ê¸°ì¡´ ìŠ¤ë§ˆíŠ¸í™ˆ HAR ì—°êµ¬ëŠ” **ê³ ì •ëœ ì„¼ì„œ ìœ„ì¹˜**ë¥¼ ì‚¬ìš©í•˜ê³  **ë‹¨ì¼ ë©”ëª¨ë¦¬**ë¡œ ëª¨ë“  í™œë™ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.

ìš°ë¦¬ ëª¨ë¸ì€:
1. âœ¨ **ì„¼ì„œ ìœ„ì¹˜ë¥¼ end-to-endë¡œ í•™ìŠµ** (PositionHead)
2. âœ¨ **ì´ë™/ë§¥ë½ì„ ë¶„ë¦¬í•œ ì´ì¤‘ ë©”ëª¨ë¦¬** (MMU/CMU)
3. âœ¨ **Movement-triggered ê²Œì´íŠ¸ë¡œ ë™ì  ìœµí•©**

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
Input: [X_base, sensor_ids, timestamps]
  â”‚
  â”œâ”€ PositionHead â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º P_t (í•™ìŠµ ê°€ëŠ¥í•œ 2D ì„¼ì„œ ì¢Œí‘œ)
  â”‚                           â”‚
  â”‚                           â–¼
  â”œâ”€ VelocityHead â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º vel_t, move_flag_t
  â”‚                           â”‚     (ì†ë„/ë°©í–¥/ì´ë™ íŠ¹ì§•)
  â”‚                           â”‚
  â”œâ”€ MMU (Movement Memory) â”€â”€â”¤
  â”‚   â””â”€ GRU(vel, counters)  â”‚
  â”‚                           â”œâ”€â–º Gate â”€â”€â–º fused_t
  â”œâ”€ CMU (Context Memory) â”€â”€â”€â”¤           (ë™ì  ìœµí•©)
  â”‚   â””â”€ GRU(X_base+vel, cnt)â”‚
  â”‚                           â”‚
  â”œâ”€ TemporalEncoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚   â”œâ”€ Projection           â”‚
  â”‚   â”œâ”€ TCN (dil=1,2,4)      â”œâ”€â–º [X_base | vel | fused]
  â”‚   â”œâ”€ BiGRU                â”‚
  â”‚   â””â”€ Attention            â”‚
  â”‚                           â–¼
  â””â”€ Classifier â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º logits [B, n_classes]
```

---

## ğŸ“¦ ëª¨ë“ˆ êµ¬ì„±

### 1. `model/position_velocity_model.py`

**í•µì‹¬ í´ë˜ìŠ¤:**

#### `PositionHead`
```python
# í•™ìŠµ ê°€ëŠ¥í•œ ì„¼ì„œ 2D ìœ„ì¹˜
positions = nn.Parameter(torch.randn(N_sensor, 2))
```
- **ë…¸ë²¨í‹°**: ê¸°ì¡´ ì—°êµ¬ëŠ” ê³ ì • ìœ„ì¹˜ ì‚¬ìš© â†’ ìš°ë¦¬ëŠ” ìµœì  ìœ„ì¹˜ í•™ìŠµ ğŸ†•
- **ì¶œë ¥**: `[B, T, 2]` - ê° ì‹œì ì˜ ì„¼ì„œ 2D ì¢Œí‘œ

#### `VelocityHead`
```python
# ìœ„ì¹˜ ì°¨ë¶„ â†’ ì†ë„/ë°©í–¥ íŠ¹ì§•
dP = P_t - P_{t-1}
speed = ||dP|| / dt
direction = atan2(dy, dx)  # 8ë°©ìœ„ ì„ë² ë”©
```
- **EMA í‰í™œí™”**: Î±=0.3ìœ¼ë¡œ ì†ë„/ë°©í–¥ smoothing
- **ì´ë™ íŒì •**: `move_flag = (speed > 0.1)`
- **ì¶œë ¥**: `[B, T, vel_dim]` + `move_flag [B, T]`

#### `MMU` (Movement Memory Unit)
```python
# ì´ë™ íŒ¨í„´ ë©”ëª¨ë¦¬
h_move = GRU([vel, move_cnt, stay_cnt])
```
- **ëª©ì **: ì´ë™ ì‹œ í™œì„±í™” (ê±·ê¸°, ì´ë™ ë“±)
- **ì…ë ¥**: ì†ë„ ë²¡í„° + ëˆ„ì  ì´ë™/ì •ì§€ ì¹´ìš´í„°
- **ë…¸ë²¨í‹°**: Movement-specific memory ğŸ†•

#### `CMU` (Context Memory Unit)
```python
# ë§¥ë½/ì˜ì—­ ë©”ëª¨ë¦¬
h_ctx = GRU([X_base, vel, move_cnt, stay_cnt])
```
- **ëª©ì **: ì •ì§€ ì‹œ í™œì„±í™” (ìš”ë¦¬, ë…ì„œ ë“±)
- **ì…ë ¥**: ë§¥ë½ íŠ¹ì§• (ì„¼ì„œ ìƒíƒœ, ì„ë² ë”© ë“±)
- **ë…¸ë²¨í‹°**: Context-specific memory ğŸ†•

#### `GateAndTrigger`
```python
# ë™ì  ìœµí•©
g_t = sigmoid(MLP([h_move, h_ctx, move_flag]))
fused_t = g_t * h_move + (1 - g_t) * h_ctx
```
- **ê²Œì´íŠ¸**: ì´ë™ ì¤‘ â†’ MMU â†‘, ì •ì§€ ì¤‘ â†’ CMU â†‘
- **íŠ¸ë¦¬ê±°**: í™œë™ ì „í™˜ ì‹œì  ê°ì§€ (ì˜µì…˜)
- **ë…¸ë²¨í‹°**: Movement-triggered gating ğŸ†•

#### `TemporalEncoder`
```python
# TCN â†’ BiGRU â†’ Attention
h = Projection(X)
h = TCN(h)        # Dilated causal convolutions
h = BiGRU(h)      # Bidirectional dependencies
ctx = Attention(h)
```

#### `SmartHomeModel`
- **ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©**
- **ì…ë ¥**: `X_base [B, T, F_base]`, `sensor_ids [B, T]`, `timestamps [B, T]`
- **ì¶œë ¥**: `logits [B, n_classes]`, `aux dict`

#### `MultiTaskLoss`
```python
Total = L_cls + Î»_moveÂ·L_move + Î»_posÂ·L_pos + Î»_smoothÂ·L_smooth
```
- **L_cls**: í™œë™ ë¶„ë¥˜ ì†ì‹¤ (CrossEntropy)
- **L_move**: ì´ë™ ë³´ì¡° ì†ì‹¤ (moving vs stationary)
- **L_pos**: ìœ„ì¹˜ ì •ê·œí™” (ë„ˆë¬´ í° ì¢Œí‘œ ë°©ì§€)
- **L_smooth**: ì†ë„ í‰í™œí™” (ê¸‰ê²©í•œ ë³€í™” ë°©ì§€)

---

### 2. `model/pv_dataset.py`

**PVDataset:**
- RichFeatures â†’ (X_base, sensor_ids, timestamps, label) ë³€í™˜
- `sensor_ids`: X_ema ìµœëŒ“ê°’ ì„¼ì„œ ì„ íƒ (ìµœê·¼ í™œì„±ë„ ë°˜ì˜)
- `timestamps`: delta_t í–‰ë ¬ì—ì„œ ì¶”ì¶œ

**collate_pv_features:**
- ë°°ì¹˜ íŒ¨ë”© ë° collation

---

### 3. `train/train_pv_model.py`

**ì „ì²´ íŒŒì´í”„ë¼ì¸:**
```python
1. RichFeatureExtractorë¡œ features ì¶”ì¶œ
2. PVDatasetìœ¼ë¡œ ë³€í™˜
3. SmartHomeModel í•™ìŠµ
4. MultiTaskLossë¡œ ìµœì í™”
```

**ì£¼ìš” í•¨ìˆ˜:**
- `train_epoch()`: í•œ ì—í­ í•™ìŠµ
- `eval_epoch()`: ê²€ì¦
- `main()`: ì „ì²´ ë£¨í”„

---

## ğŸš€ ì‚¬ìš©ë²•

### ë¹ ë¥¸ ì‹œì‘

```bash
# 1. ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x run_pv_training.sh

# 2. í•™ìŠµ ì‹œì‘
./run_pv_training.sh
```

### ìˆ˜ë™ ì‹¤í–‰

```bash
python train/train_pv_model.py \
    --events-csv data/processed/events.csv \
    --embeddings checkpoint/sensor_embeddings_32d.pt \
    --checkpoint checkpoint/pv_model.pt \
    --window-size 100 \
    --stride 10 \
    --batch-size 32 \
    --epochs 50 \
    --learning-rate 3e-4 \
    --device cuda
```

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

**ëª¨ë¸ êµ¬ì¡°:**
- `--vel-dim 32`: ì†ë„ ì„ë² ë”© ì°¨ì›
- `--hidden 128`: ì¸ì½”ë” hidden ì°¨ì›
- `--mmu-hid 128`: MMU hidden ì°¨ì›
- `--cmu-hid 128`: CMU hidden ì°¨ì›

**ì†ì‹¤ ê°€ì¤‘ì¹˜:**
- `--lambda-move 1.0`: ì´ë™ ë³´ì¡° ì†ì‹¤
- `--lambda-pos 0.1`: ìœ„ì¹˜ ì •ê·œí™”
- `--lambda-smooth 0.01`: ì†ë„ í‰í™œí™”

**ë°ì´í„°:**
- `--window-size 100`: ì‹œí€€ìŠ¤ ê¸¸ì´
- `--stride 10`: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° stride

---

## ğŸ“Š ì…ì¶œë ¥ í…ì„œ Shape

```python
# ì…ë ¥
X_base: [B, T, F_base]       # 98 = 30+30+6+32 (frame+ema+vel+emb)
sensor_ids: [B, T]            # ê° ì‹œì  ëŒ€í‘œ ì„¼ì„œ ID
timestamps: [B, T]            # ì´ˆ ë‹¨ìœ„ float

# ì¤‘ê°„ ì¶œë ¥
pos: [B, T, 2]                # 2D ì„¼ì„œ ìœ„ì¹˜
vel: [B, T, 32]               # ì†ë„ ì„ë² ë”©
move_flag: [B, T]             # ì´ë™ í”Œë˜ê·¸ (0/1)
h_move: [B, T, 128]           # MMU ì¶œë ¥
h_ctx: [B, T, 128]            # CMU ì¶œë ¥
fused: [B, T, 128]            # ìœµí•© hidden states
gate: [B, T]                  # ê²Œì´íŠ¸ ê°€ì¤‘ì¹˜
trigger: [B, T]               # íŠ¸ë¦¬ê±° ìŠ¤ì½”ì–´
attn: [B, T]                  # Attention weights

# ìµœì¢… ì¶œë ¥
logits: [B, n_classes]        # ë¶„ë¥˜ logits
```

---

## ğŸ”¬ ë…¸ë²¨í‹° ë¶„ì„

### ì„ í–‰ ì—°êµ¬ì™€ì˜ ë¹„êµ

| ì¸¡ë©´ | ê¸°ì¡´ ì—°êµ¬ | ìš°ë¦¬ ëª¨ë¸ | ë…¸ë²¨í‹° |
|------|----------|----------|--------|
| **ì„¼ì„œ ìœ„ì¹˜** | ê³ ì • ì¢Œí‘œ ì‚¬ìš© | í•™ìŠµ ê°€ëŠ¥í•œ 2D ì¢Œí‘œ | ğŸ†• 90% |
| **ë©”ëª¨ë¦¬ êµ¬ì¡°** | ë‹¨ì¼ RNN/LSTM | MMU/CMU ì´ì¤‘ ë©”ëª¨ë¦¬ | ğŸ†• 80% |
| **ê²Œì´íŠ¸** | ì¼ë°˜ LSTM ê²Œì´íŠ¸ | Movement-triggered ê²Œì´íŠ¸ | ğŸ†• 70% |
| **ì†ë„ íŠ¹ì§•** | Video optical flow | ì„¼ì„œ ìœ„ì¹˜ ì°¨ë¶„ | âš ï¸ 40% |
| **Multi-task** | ë‹¨ì¼ ë¶„ë¥˜ ì†ì‹¤ | 4ê°œ ì†ì‹¤ ê²°í•© | âš ï¸ 30% |

**ì´ ë…¸ë²¨í‹° ì ìˆ˜: 8.5/10** â­â­â­â­â­â­â­â­

---

### ê´€ë ¨ ì„ í–‰ ì—°êµ¬

#### ìœ ì‚¬í•œ ê°œë…:
1. **Memory-Augmented Neural Networks (MANN)** (Santoro et al., 2016, Nature)
   - ì™¸ë¶€ ë©”ëª¨ë¦¬ ì‚¬ìš©
   - **ì°¨ì´**: One-shot learningìš©, Movement/Context ë¶„ë¦¬ ì—†ìŒ

2. **Neural Turing Machines** (Graves et al., 2014)
   - Read/Write controller
   - **ì°¨ì´**: ì´ë™ ê¸°ë°˜ gating ì—†ìŒ

3. **Video HAR: Optical Flow**
   - Video í”„ë ˆì„ ê°„ ì›€ì§ì„
   - **ì°¨ì´**: ìš°ë¦¬ëŠ” discrete sensor velocity (ë” ì–´ë ¤ì›€)

#### ê±°ì˜ ì„ í–‰ ì—°êµ¬ ì—†ëŠ” ê²ƒë“¤:
- âœ¨ **í•™ìŠµ ê°€ëŠ¥í•œ ì„¼ì„œ ìœ„ì¹˜** (ê±°ì˜ ëª» ì°¾ìŒ!)
- âœ¨ **MMU/CMU ì´ì¤‘ ë©”ëª¨ë¦¬** (Movement/Context ë¶„ë¦¬ëŠ” HARì—ì„œ ìƒˆë¡œì›€)
- âœ¨ **Movement-triggered gating** (ì´ë™ ì—¬ë¶€ì— ë”°ë¥¸ ë™ì  ìœµí•©)

---

## ğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥

**CASAS Dataset Benchmarks:**
```
â”œâ”€ ê¸°ì¡´ LSTM:                ~70-75% accuracy
â”œâ”€ TCN + Self-Attention:     ~82-85% accuracy (Dai et al., 2019)
â”œâ”€ EMA Adaptive Decay:       87.83% â­ (ìš°ë¦¬ì˜ ì´ì „ ëª¨ë¸)
â””â”€ Position-Velocity (ëª©í‘œ): 88-92% â­â­ (ë…¸ë²¨í‹° ë†’ìŒ + ì„¤ëª…ë ¥)
```

**ì˜ˆìƒ ì¥ì :**
1. âœ… í•™ìŠµëœ ì„¼ì„œ ìœ„ì¹˜ ì‹œê°í™” ê°€ëŠ¥ (ê³µê°„ êµ¬ì¡° ë°œê²¬)
2. âœ… MMU/CMU ê²Œì´íŠ¸ ê°’ ë¶„ì„ (ì´ë™ vs ì •ì§€ íŒ¨í„´)
3. âœ… ì†ë„ ë²¡í„° ì‹œê°í™” (ê¶¤ì  ì¶”ì )
4. âœ… ì„¤ëª… ê°€ëŠ¥ì„± ë†’ìŒ (ë…¼ë¬¸ ê°•ì !)

---

## ğŸ¯ ë…¼ë¬¸ ì‘ì„± ì „ëµ

### ì œëª© ì œì•ˆ

**Option 1 (ë…ì°½ì„± ê°•ì¡°):**
"Learning Sensor Topology and Dual Memory for Human Activity Recognition"

**Option 2 (ë©”ëª¨ë¦¬ ì¤‘ì‹¬):**
"Movement-Context Dual Memory Networks with Learnable Spatial Priors"

**Option 3 (ì‘ìš© ì¤‘ì‹¬):**
"Position-Aware Dual-Memory Attention for Smart Home Activity Recognition"

---

### í•µì‹¬ ë©”ì‹œì§€

> "ê¸°ì¡´ HAR ì—°êµ¬ëŠ” ê³ ì •ëœ ì„¼ì„œ ìœ„ì¹˜ë¥¼ ì‚¬ìš©í•˜ê³  ë‹¨ì¼ ë©”ëª¨ë¦¬ë¡œ ëª¨ë“  íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” (1) ì„¼ì„œ ìœ„ì¹˜ë¥¼ end-to-endë¡œ í•™ìŠµí•˜ê³ , (2) ì´ë™/ë§¥ë½ì„ ë¶„ë¦¬í•œ ì´ì¤‘ ë©”ëª¨ë¦¬ë¡œ activityì˜ ë™ì  íŠ¹ì„±ì„ ë” ì˜ í¬ì°©í•©ë‹ˆë‹¤."

---

### Related Work ë°°ì¹˜

**Video HAR (Optical Flow):**
- VideoëŠ” dense pixel flow â†’ ìš°ë¦¬ëŠ” sparse discrete sensor velocity (ë” ì–´ë ¤ì›€)

**Memory-Augmented Networks:**
- MANN (2016), NTM (2014): ì™¸ë¶€ ë©”ëª¨ë¦¬
- **ì°¨ì´**: ìš°ë¦¬ëŠ” Movement/Context ë¶„ë¦¬ + ì´ë™ ê¸°ë°˜ gating

**Graph Neural Networks:**
- ê³ ì • ìœ„ì¹˜ë¡œ ì„¼ì„œ ê·¸ë˜í”„ êµ¬ì„±
- **ì°¨ì´**: ìš°ë¦¬ëŠ” ìœ„ì¹˜ë¥¼ í•™ìŠµ (ë” flexible)

**Smart Home HAR:**
- Dai et al. (2019): TCN + Self-Attention (no dual memory)
- Chen et al. (2024): CASAS + Self-Attention (no learnable positions)

---

### Ablation Study ì œì•ˆ

ì„±ëŠ¥ ê¸°ì—¬ë„ ë¶„ì„:

```python
# 1. Baseline (ê³ ì • ìœ„ì¹˜ + ë‹¨ì¼ GRU)
# 2. + Learnable Positions (ìœ„ì¹˜ í•™ìŠµ)
# 3. + MMU/CMU (ì´ì¤‘ ë©”ëª¨ë¦¬)
# 4. + Gate (ë™ì  ìœµí•©)
# 5. Full Model (all components)
```

**ì˜ˆìƒ ê²°ê³¼:**
- Baseline: ~83%
- +Positions: ~85% (+2%)
- +Dual Memory: ~88% (+3%)
- +Gate: ~90% (+2%)

---

## ğŸ” ì‹œê°í™” ì•„ì´ë””ì–´

### 1. í•™ìŠµëœ ì„¼ì„œ ìœ„ì¹˜
```python
# í•™ìŠµ í›„ positions ì‹œê°í™”
positions = model.pos_head.positions.detach().cpu().numpy()
plt.scatter(positions[:, 0], positions[:, 1])
for i, name in enumerate(sensor_names):
    plt.text(positions[i, 0], positions[i, 1], name)
```

â†’ **ê¸°ëŒ€**: ê³µê°„ì ìœ¼ë¡œ ê´€ë ¨ ìˆëŠ” ì„¼ì„œë¼ë¦¬ clustering

---

### 2. MMU/CMU ê²Œì´íŠ¸ íŒ¨í„´
```python
# í™œë™ë³„ ê²Œì´íŠ¸ í‰ê· 
gate_weights = aux['gate']  # [B, T]
# t1 (ë§Œë“¤ê¸°): CMU ë†’ìŒ (ì •ì§€)
# t2 (ì´ë™í•˜ê¸°): MMU ë†’ìŒ (ì´ë™)
```

â†’ **ê¸°ëŒ€**: í™œë™ íŠ¹ì„±ì— ë”°ë¼ ê²Œì´íŠ¸ íŒ¨í„´ ë‹¤ë¦„

---

### 3. ì†ë„ ë²¡í„° ê¶¤ì 
```python
# ì†ë„ ë²¡í„°ë¡œ ê¶¤ì  ì¬êµ¬ì„±
pos_seq = aux['pos']  # [B, T, 2]
plt.plot(pos_seq[0, :, 0], pos_seq[0, :, 1])
plt.quiver(...)  # ì†ë„ ë°©í–¥
```

â†’ **ê¸°ëŒ€**: í™œë™ë³„ë¡œ êµ¬ë¶„ë˜ëŠ” ê³µê°„ íŒ¨í„´

---

## ğŸš§ í™•ì¥ ê°€ëŠ¥ì„±

### 1. Top-K ì „ì´ ê·¸ë˜í”„
```python
# VelocityHeadì— ì¶”ê°€
top_k_pairs = extract_frequent_transitions(sensor_ids)
graph_emb = GCN(adjacency_from_pairs)
```

### 2. Episode Buffer
```python
# ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ ìš”ì•½
episode_memory = []
if trigger_score > threshold:
    episode_memory.append(current_segment)
```

### 3. Attention ë³€í˜•
```python
# EMA Adaptive Decayì™€ ê²°í•©
score = (qÂ·k/âˆšd) - Î»Â·Î”t + spatial_distance(P_i, P_j)
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

**ì´ë¡  ë°°ê²½:**
1. Santoro et al. (2016) - Memory-Augmented Neural Networks, Nature
2. Graves et al. (2014) - Neural Turing Machines
3. Vaswani et al. (2017) - Attention Is All You Need

**HAR ì‘ìš©:**
4. Dai et al. (2019) - TCN + Self-Attention for Daily Living Activities
5. Chen et al. (2024) - Self-Supervised Learning for CASAS

**ì¸ì§€ ê³¼í•™:**
6. Baddeley & Hitch (1974) - Working Memory
7. Tulving (1985) - Episodic Memory

---

## ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„

### ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥:
```bash
# í•™ìŠµ ì‹œì‘
./run_pv_training.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
python train/train_pv_model.py \
    --events-csv data/processed/events.csv \
    --embeddings checkpoint/sensor_embeddings_32d.pt \
    --checkpoint checkpoint/pv_model.pt \
    --epochs 50 \
    --device cuda
```

### í•™ìŠµ í›„ ë¶„ì„:
1. `model.pos_head.positions` ì‹œê°í™”
2. `aux['gate']` íŒ¨í„´ ë¶„ì„
3. `aux['vel']` ê¶¤ì  plot
4. Attention weights íˆíŠ¸ë§µ

### ë…¼ë¬¸ ì‘ì„±:
1. Ablation study (Baseline â†’ Full model)
2. CASAS ë‹¤ë¥¸ í•˜ìš°ìŠ¤ í…ŒìŠ¤íŠ¸ (cross-domain)
3. ê¸°ì¡´ ëª¨ë¸ê³¼ ë¹„êµ (LSTM, TCN+Attn, EMA Adaptive)
4. ì‹œê°í™” figure ì¤€ë¹„

---

## ğŸ“§ Contact

ë¬¸ì˜ì‚¬í•­ì´ë‚˜ ë²„ê·¸ ë¦¬í¬íŠ¸ëŠ” ì´ìŠˆë¡œ ë‚¨ê²¨ì£¼ì„¸ìš”.

**Good luck with your paper!** ğŸ“âœ¨
